# Persona

Persona is an AI companion app that lets you chat with custom personality constructs, tweak model behavior, and experiment with memory controls. Think of it as a sandbox for testing how tone, prompts, and retrieval settings change the feel of an AI chat experience.

> Prototype-first mindset: we prioritize showing the experience quickly, then layering depth.

## Features

- **Multiple characters** â€“ Sam (friend), Therapist, Coding Guru, Creative Writer, Data Analyst, or create custom personas via the Character Builder
- **Local-first AI** â€“ LM Studio for chat models, KoboldCpp for embeddingsâ€”no cloud dependency required
- **RAG memory system** â€“ Upload documents, auto-index with BGE-M3 embeddings, retrieve relevant context per conversation
- **Model dial-ins** â€“ Swap between local and cloud models, adjust temperature, per-character and per-chat overrides
- **Streaming chat** â€“ Real-time message streaming via AI SDK v5
- **Character versioning** â€“ Snapshots, checkpoints, import/export for portability

## Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Next.js 16 App                           â”‚
â”‚                   (localhost:3000)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                             â”‚
              â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      LM Studio          â”‚   â”‚      KoboldCpp          â”‚
â”‚    (localhost:1234)     â”‚   â”‚    (localhost:5001)     â”‚
â”‚                         â”‚   â”‚                         â”‚
â”‚  â€¢ Chat completions     â”‚   â”‚  â€¢ BGE-M3 embeddings    â”‚
â”‚  â€¢ OpenAI-compatible    â”‚   â”‚  â€¢ 1024 dimensions      â”‚
â”‚  â€¢ Local LLMs           â”‚   â”‚  â€¢ Docker container     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                             â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      PostgreSQL 18      â”‚
              â”‚    (localhost:5432)     â”‚
              â”‚                         â”‚
              â”‚  â€¢ pgvector extension   â”‚
              â”‚  â€¢ Conversations        â”‚
              â”‚  â€¢ Characters           â”‚
              â”‚  â€¢ Memory items         â”‚
              â”‚  â€¢ Background jobs      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Tech Stack

| Layer | Technology |
|-------|------------|
| Frontend | Next.js 16, React 19, Tailwind v4, shadcn/ui |
| State | Zustand + Zod v4 validation |
| Database | PostgreSQL 18 + pgvector + DrizzleORM |
| Chat | AI SDK v5, LM Studio (local), OpenAI (cloud fallback) |
| Embeddings | KoboldCpp + BGE-M3 (local), OpenAI (fallback) |
| Jobs | pg-boss (PostgreSQL-backed queue) |

## Quick Start

### Prerequisites

- Node.js 20+
- pnpm
- Docker Desktop
- LM Studio (for chat models)

### 1. Clone and install

```bash
git clone <repo-url>
cd persona
pnpm install
cp .env.example .env
```

### 2. Start infrastructure

```bash
docker compose up -d
```

This starts:
- **PostgreSQL** on port 5432 (with pgvector)
- **KoboldCpp** on port 5001 (downloads BGE-M3 model on first run, ~438MB)

Wait for KoboldCpp to be ready:
```bash
# Check health
curl http://localhost:5001/api/v1/models
```

### 3. Initialize database

```bash
pnpm db:push    # Apply schema
pnpm db:seed    # Seed default characters
```

### 4. Start LM Studio

1. Open LM Studio
2. Load a chat model (e.g., Qwen3-8B, Llama 3.2)
3. Start the local server on port 1234

### 5. Run the app

```bash
# Terminal 1: Next.js dev server
pnpm dev

# Terminal 2: Background job worker
pnpm worker:dev
```

Open <http://localhost:3000>

## Environment Variables

```bash
# Database
DATABASE_URL="postgresql://persona:persona_dev@localhost:5432/persona_dev"

# LM Studio (chat models)
LM_STUDIO_BASE_URL="http://localhost:1234/v1"

# KoboldCpp (embeddings) - Docker handles this
EMBEDDING_BASE_URL="http://localhost:5001/v1"

# Optional: OpenAI fallback
OPENAI_API_KEY="sk-..."
```

## Scripts

| Command | Purpose |
|---------|---------|
| `pnpm dev` | Start Next.js dev server |
| `pnpm build` | Production build |
| `pnpm worker:dev` | Start background job worker |
| `pnpm db:push` | Push schema to database |
| `pnpm db:seed` | Seed default data |
| `pnpm db:studio` | Open Drizzle Studio |
| `pnpm test` | Run test suite |
| `pnpm lint` | Run oxlint |

## Project Structure

```text
app/                    # Next.js App Router
â”œâ”€â”€ api/                # API routes (chat, conversations, characters, etc.)
â”œâ”€â”€ characters/         # Character library and builder
â”œâ”€â”€ knowledge-base/     # Knowledge base management
components/             # React components
â”œâ”€â”€ chat/               # Chat interface
â”œâ”€â”€ sidebar-left/       # Conversation list
â”œâ”€â”€ sidebar-right/      # Character & memory panel
lib/
â”œâ”€â”€ db/                 # DrizzleORM schema and client
â”œâ”€â”€ rag/                # RAG system (embedding, retrieval, indexing)
â”œâ”€â”€ jobs/               # pg-boss job queue
â”œâ”€â”€ providers/          # LM Studio, OpenAI providers
config/
â”œâ”€â”€ models.json         # Available chat models
â”œâ”€â”€ rag.json            # RAG configuration
â”œâ”€â”€ characters/         # Built-in character templates
```

## RAG System

The RAG (Retrieval-Augmented Generation) system provides per-character knowledge bases:

1. **Upload** â€“ Files are stored locally, metadata in PostgreSQL
2. **Index** â€“ Background worker chunks files and generates embeddings via KoboldCpp
3. **Retrieve** â€“ On each chat turn, relevant chunks are fetched via pgvector similarity search
4. **Inject** â€“ Retrieved context is added to the system prompt

Configuration in `config/rag.json`:
- Default top-K: 8 chunks per query
- Embedding model: BGE-M3 (1024 dimensions)
- Chunk size: 500 tokens with 50 token overlap

## Roadmap

See [TODO.md](./TODO.md) for detailed progress. Current phase: **Phase 3 â€“ Dev-Grade Memory Controls**

- âœ… Phase 0: Foundation & UI shell
- âœ… Phase 1: Core UX (chats, characters, RAG basics)
- âœ… Phase 2: Character Studio & Import/Export
- ğŸŸ¡ Phase 3: Memory controls & forgetting tools
- â¬œ Phase 4: Voice I/O and local tools
- â¬œ Phase 5: Productivity workflows

## Contributing

Issues and PRs welcome. Keep commits tight and descriptive.
